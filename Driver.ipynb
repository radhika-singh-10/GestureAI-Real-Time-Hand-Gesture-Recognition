{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Model\n",
    "!python3 train.py --config configs/config_quick_testing.json --use_gpu=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train.py --config configs/config_quick_testing.json --use_gpu=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train.py --config configs/config_quick_testing.json --use_gpu=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./20bn-jester-v1/annotations/jester-v1-train-quick-testing.csv',delimiter=';',header=None)\n",
    "data.head()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv('./20bn-jester-v1/annotations/jester-v1-validation.csv', delimiter=';', header=None)\n",
    "\n",
    "# List of classes you want to select\n",
    "classes_to_select = ['Swiping Left', 'Swiping Right', 'Swiping Down', 'Swiping Up', 'Doing other things']  # Add the classes you want to select\n",
    "\n",
    "# Assuming the class label is in a specific column, let's say column 1 (adjust the column number accordingly)\n",
    "class_column = 1  # Adjust the column number if the class label is in a different column\n",
    "\n",
    "# Filter rows where the class label is in the list of classes to select\n",
    "filtered_data = data[data[class_column].isin(classes_to_select)]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(filtered_data)\n",
    "\n",
    "# filtered_data = filtered_data.sample(2000)\n",
    "print(filtered_data.shape)\n",
    "filtered_data.to_csv('5_class_validation_data.csv',header=False,index=False,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/atharvamusale/Downloads/DL_Project/CVND---Gesture-Recognition/20bn-jester-v1/annotations/5_class_training_data.csv\")\n",
    "print(data.shape)\n",
    "print(data.head())\n",
    "# data['1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train.py --config configs/config_quick_testing.json --use_gpu=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_folder = \"/Users/atharvamusale/Downloads/DL_Project/CVND---Gesture-Recognition/data2/20bn-jester-v1\"\n",
    "\n",
    "def check_empty_folders(root):\n",
    "    empty_folders = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        for dirname in dirnames:\n",
    "            folder_path = os.path.join(dirpath, dirname)\n",
    "            if not os.listdir(folder_path):\n",
    "                print(f\"The folder '{folder_path}' is empty.\")\n",
    "                empty_folders.append(dirname)  # Append the folder number to the list\n",
    "            else:\n",
    "                # print(f\"The folder '{folder_path}' is not empty.\")\n",
    "                continue\n",
    "    return empty_folders\n",
    "\n",
    "empty_folders_list = check_empty_folders(root_folder)\n",
    "print(\"Empty folders:\", empty_folders_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the list to a pickle file\n",
    "pickle_file = \"empty_folders_list.pickle\"\n",
    "with open(pickle_file, 'wb') as f:\n",
    "    pickle.dump(empty_folders_list, f)\n",
    "\n",
    "print(f\"Empty folders list saved to '{pickle_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Define the path to the pickle file\n",
    "pickle_file_path = \"/Users/atharvamusale/Downloads/DL_Project/CVND---Gesture-Recognition/empty_folders_list.pickle\"\n",
    "\n",
    "# Read the content of the pickle file\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    empty_folders_list = pickle.load(f)\n",
    "\n",
    "# Print the content of the pickle file\n",
    "print(\"Empty folders list:\", empty_folders_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_folders_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace with the path to your actual CSV file.\n",
    "csv_path = './20bn-jester-v1/annotations/5_class_training_data.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame. Assuming there is no header in the CSV.\n",
    "df = pd.read_csv(csv_path, header=None,delimiter=';')\n",
    "\n",
    "# Filter out the rows where the first column is in 'empty_folders_list'.\n",
    "filtered_df = df[~df[0].isin(empty_folders_list)]\n",
    "\n",
    "# Replace with your desired output file name.\n",
    "filtered_csv_path = '5_class_training_data_filtered.csv'\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file.\n",
    "filtered_df.to_csv(filtered_csv_path, index=False, header=False,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace with the path to your actual CSV file.\n",
    "csv_path = './20bn-jester-v1/annotations/5_class_validation_data.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame. Assuming there is no header in the CSV.\n",
    "df = pd.read_csv(csv_path, header=None,delimiter=';')\n",
    "\n",
    "# Filter out the rows where the first column is in 'empty_folders_list'.\n",
    "filtered_df = df[~df[0].isin(empty_folders_list)]\n",
    "\n",
    "# Replace with your desired output file name.\n",
    "filtered_csv_path = '5_class_valid_data_filtered.csv'\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file.\n",
    "filtered_df.to_csv(filtered_csv_path, index=False, header=False,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# # Load the list from the pickle file\n",
    "# with open('your_empty_folders_list_path.pickle', 'rb') as file:\n",
    "#     empty_folders_list = pickle.load(file)\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('./20bn-jester-v1/annotations/5classvalid10000.csv', header=None, sep=';')\n",
    "\n",
    "# Separate the numbers and match against the 'empty_folders_list'\n",
    "df[0] = df[0].astype(str)  # Convert to string to ensure string operations can be performed\n",
    "filtered_df = df[~df[0].isin(empty_folders_list)]\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "filtered_df.to_csv('filtered_val_data.csv', index=False, header=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing video on trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/Users/atharvamusale/Downloads/DL_Project/CVND---Gesture-Recognition/data2/20bn-jester-v1/63552\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision.transforms import Compose, CenterCrop, ToTensor, Normalize\n",
    "from model import GestureDetection\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the path where images are stored and the output video file path\n",
    "image_folder = '/Users/atharvamusale/Downloads/DL_Project/CVND---Gesture-Recognition/data2/20bn-jester-v1/104138'\n",
    "video_path = '/Users/atharvamusale/Downloads/DL_Project/output_video.mp4'\n",
    "\n",
    "# Specify the desired output video dimensions and frame rate\n",
    "frame_width = 1920  # Example width or adapt to your images\n",
    "frame_height = 1080  # Example height or adapt to your images\n",
    "fps = 30  # Frames per second\n",
    "\n",
    "# Create a VideoWriter object using the specified settings\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Codec used to compress the frames\n",
    "out = cv2.VideoWriter(video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# List and sort the files in the image directory\n",
    "files = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
    "files.sort()\n",
    "\n",
    "# Loop through each file, read the image, resize it, and write it to the output video\n",
    "for filename in files:\n",
    "    img = cv2.imread(filename)\n",
    "    if img is None:\n",
    "        continue  # Skip any files that aren't valid images\n",
    "    img = cv2.resize(img, (frame_width, frame_height))  # Resize the image to fit the video dimensions\n",
    "    out.write(img)  # Write the frame to the video\n",
    "\n",
    "# Release the VideoWriter object and close all windows\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f'Video saved at {video_path}')\n",
    "\n",
    "\n",
    "label_mapping_path = './20bn-jester-v1/annotations/jester-v1-labels-quick-testing copy.csv'\n",
    "\n",
    "\n",
    "def load_label_mapping(label_mapping_path):\n",
    "    label_mapping = {}\n",
    "    with open(label_mapping_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            label = line.strip()\n",
    "            label_mapping[i] = label\n",
    "    return label_mapping\n",
    "\n",
    "label_mapping = load_label_mapping(label_mapping_path)\n",
    "\n",
    "\n",
    "def preprocess_frames(frames, transform):\n",
    "    processed_frames = [transform(Image.fromarray(frame)) for frame in frames]\n",
    "    frame_stack = torch.stack(processed_frames, dim=1)\n",
    "    return frame_stack.unsqueeze(0)\n",
    "# Load the model\n",
    "model = GestureDetection(num_classes=7)\n",
    "checkpoint = torch.load('./trainings/jpeg_model/7_classes/v2/checkpoint.pth.tar', map_location='cpu')\n",
    "state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint\n",
    "\n",
    "state_dict_modified = {}\n",
    "for key, value in state_dict.items():\n",
    "    if key.startswith('module.'):\n",
    "        new_key = key[len('module.'):]  # Remove the 'module.' prefix\n",
    "        state_dict_modified[new_key] = value\n",
    "    else:\n",
    "        state_dict_modified[key] = value\n",
    "\n",
    "model.load_state_dict(state_dict_modified)\n",
    "model.eval()\n",
    "\n",
    "# Define the transformation\n",
    "transform = Compose([\n",
    "    CenterCrop(84),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load label mapping\n",
    "def load_label_mapping(label_mapping_path):\n",
    "    label_mapping = {}\n",
    "    with open(label_mapping_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            label = line.strip()\n",
    "            label_mapping[i] = label\n",
    "    return label_mapping\n",
    "\n",
    "label_mapping = load_label_mapping('./20bn-jester-v1/annotations/jester-v1-labels-quick-testing copy.csv')\n",
    "\n",
    "# Video setup\n",
    "video_path = '/Users/atharvamusale/Downloads/DL_Project/output_video.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_sequence = []\n",
    "sequence_length = 16\n",
    "\n",
    "# Setup video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') # or 'XVID'\n",
    "out_video_path = '/Users/atharvamusale/Downloads/DL_Project/output_predictions_video.mp4'\n",
    "out = cv2.VideoWriter(out_video_path, fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if len(frame_sequence) < sequence_length:\n",
    "        frame_sequence.append(frame_rgb)\n",
    "        continue\n",
    "    else:\n",
    "        frame_sequence.pop(0)\n",
    "        frame_sequence.append(frame_rgb)\n",
    "\n",
    "    # Process the frames and predict\n",
    "    frame_processed = preprocess_frames(frame_sequence, transform)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(frame_processed)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    predicted_gesture = label_mapping[predicted.item()]\n",
    "\n",
    "    # Add prediction text to the frame\n",
    "    cv2.putText(frame, predicted_gesture, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow('Video - Gesture Recognition', frame)\n",
    "    out.write(frame)  # Write the frame with predictions to the output video\n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Video with predictions saved to:\", out_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-27 15:04:42.300 Python[29485:5806442] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/atharvamusale/Downloads/DL_Project/CVND---Gesture-Recognition/test.py\", line 168, in <module>\n",
      "    frame_processed = preprocess_frames(frame_sequence, transform)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/atharvamusale/Downloads/DL_Project/CVND---Gesture-Recognition/test.py\", line 146, in preprocess_frames\n",
      "    processed_frames = [transform(Image.fromarray(frame)) for frame in frames]\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/atharvamusale/Downloads/DL_Project/CVND---Gesture-Recognition/test.py\", line 146, in <listcomp>\n",
      "    processed_frames = [transform(Image.fromarray(frame)) for frame in frames]\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/PIL/Image.py\", line 3122, in fromarray\n",
      "    return frombuffer(mode, size, obj, \"raw\", rawmode, 0, 1)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/PIL/Image.py\", line 3037, in frombuffer\n",
      "    return frombytes(mode, size, data, decoder_name, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/PIL/Image.py\", line 2970, in frombytes\n",
      "    im = new(mode, size)\n",
      "         ^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/PIL/Image.py\", line 2941, in new\n",
      "    return im._new(core.fill(mode, size, color))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved at /Users/atharvamusale/Downloads/DL_Project/output_video.mp4\n",
      "Video with predictions saved to: /Users/atharvamusale/Downloads/DL_Project/output_predictions_video.mp4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from torchvision.transforms import Compose, CenterCrop, ToTensor, Normalize\n",
    "from model import GestureDetection\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Define the path where images are stored and the output video file path\n",
    "image_folder = '/Users/atharvamusale/Downloads/DL_Project/CVND---Gesture-Recognition/data2/20bn-jester-v1/135719'\n",
    "video_path = '/Users/atharvamusale/Downloads/DL_Project/output_video.mp4'\n",
    "\n",
    "# Specify the desired output video dimensions and frame rate\n",
    "frame_width = 1920  # Example width or adapt to your images\n",
    "frame_height = 1080  # Example height or adapt to your images\n",
    "fps = 30  # Frames per second\n",
    "\n",
    "# Create a VideoWriter object using the specified settings\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Codec used to compress the frames\n",
    "out = cv2.VideoWriter(video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# List and sort the files in the image directory\n",
    "files = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
    "files.sort()\n",
    "\n",
    "# Loop through each file, read the image, resize it, and write it to the output video\n",
    "for filename in files:\n",
    "    img = cv2.imread(filename)\n",
    "    if img is None:\n",
    "        continue  # Skip any files that aren't valid images\n",
    "    img = cv2.resize(img, (frame_width, frame_height))  # Resize the image to fit the video dimensions\n",
    "    out.write(img)  # Write the frame to the video\n",
    "\n",
    "# Release the VideoWriter object and close all windows\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f'Video saved at {video_path}')\n",
    "\n",
    "label_mapping_path = './20bn-jester-v1/annotations/jester-v1-labels-quick-testing copy.csv'\n",
    "\n",
    "\n",
    "def load_label_mapping(label_mapping_path):\n",
    "    label_mapping = {}\n",
    "    with open(label_mapping_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            label = line.strip()\n",
    "            label_mapping[i] = label\n",
    "    return label_mapping\n",
    "\n",
    "label_mapping = load_label_mapping(label_mapping_path)\n",
    "\n",
    "\n",
    "def preprocess_frames(frames, transform):\n",
    "    processed_frames = [transform(Image.fromarray(frame)) for frame in frames]\n",
    "    frame_stack = torch.stack(processed_frames, dim=1)\n",
    "    return frame_stack.unsqueeze(0)\n",
    "\n",
    "# Load the model\n",
    "model = GestureDetection(num_classes=7)\n",
    "\n",
    "checkpoint = torch.load('./trainings/jpeg_model/7_classes/v13/checkpoint.pth.tar', map_location='cpu')\n",
    "state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint\n",
    "\n",
    "\n",
    "# Remove 'module.' prefix if present\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] if k.startswith('module.') else k  # remove `module.` prefix\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "# Load the updated state dict into the model\n",
    "model.load_state_dict(new_state_dict, strict=False)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# # Modify the state dictionary keys to match the model architecture\n",
    "# state_dict_modified = {}\n",
    "# for key, value in state_dict.items():\n",
    "#     if key.startswith('conv_layer4'):  # Modify the keys related to the added convolutional layer\n",
    "#         new_key = 'conv_layer3.' + key.split('.', 1)[1]  # Adjust the key\n",
    "#         state_dict_modified[new_key] = value\n",
    "#     else:\n",
    "#         state_dict_modified[key] = value\n",
    "\n",
    "# Define the transformation\n",
    "transform = Compose([\n",
    "    CenterCrop(84),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load label mapping\n",
    "def load_label_mapping(label_mapping_path):\n",
    "    label_mapping = {}\n",
    "    with open(label_mapping_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            label = line.strip()\n",
    "            label_mapping[i] = label\n",
    "    return label_mapping\n",
    "\n",
    "label_mapping = load_label_mapping('./20bn-jester-v1/annotations/jester-v1-labels-quick-testing copy.csv')\n",
    "\n",
    "# Video setup\n",
    "video_path = '/Users/atharvamusale/Downloads/DL_Project/output_video.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_sequence = []\n",
    "sequence_length = 16\n",
    "\n",
    "# Setup video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') # or 'XVID'\n",
    "out_video_path = '/Users/atharvamusale/Downloads/DL_Project/output_predictions_video.mp4'\n",
    "out = cv2.VideoWriter(out_video_path, fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if len(frame_sequence) < sequence_length:\n",
    "        frame_sequence.append(frame_rgb)\n",
    "        continue\n",
    "    else:\n",
    "        frame_sequence.pop(0)\n",
    "        frame_sequence.append(frame_rgb)\n",
    "\n",
    "    # Process the frames and predict\n",
    "    frame_processed = preprocess_frames(frame_sequence, transform)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(frame_processed)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    predicted_gesture = label_mapping[predicted.item()]\n",
    "\n",
    "    # Add prediction text to the frame\n",
    "    cv2.putText(frame, predicted_gesture, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow('Video - Gesture Recognition', frame)\n",
    "    out.write(frame)  # Write the frame with predictions to the output video\n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Video with predictions saved to:\", out_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
